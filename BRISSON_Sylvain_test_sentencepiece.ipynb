{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : tokenisation\n",
    "\n",
    "Test de l'entrainement de modèles de tokenisation avec le logiciel sentencepiece, depuis des corpus issus du JORF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des données\n",
    "\n",
    "Données issues du JO (JORF : Journal Officile de la République Française) : \n",
    "- lois\n",
    "- lois organiques\n",
    "- ordonnances\n",
    "- décrets\n",
    "- arrếtés\n",
    "\n",
    "On écrit l'ensemble du contenu extrait (la colonne Contenu) dans un fichier texte.\n",
    "On fait de même en séparant les contenus étant directement du Droit (entourés par des guillemets) de ceux étant auxilliaires au Droit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID texte</th>\n",
       "      <th>ID article</th>\n",
       "      <th>Nature</th>\n",
       "      <th>N° article</th>\n",
       "      <th>N° alinéa</th>\n",
       "      <th>Contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JORFTEXT000048734585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>fr/lr/loi/2023-1380/2023-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JORFTEXT000048734585</td>\n",
       "      <td>JORFVERS000048734585</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>LOI n° 2023-1380 du 30 décembre 2023 visant à ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JORFTEXT000048734585</td>\n",
       "      <td>JORFARTI000048734586</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I. — Après l'article L. 2122-19 du code généra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JORFTEXT000048734585</td>\n",
       "      <td>JORFARTI000048734586</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>« Art. L. 2122-19-1. — Pour assurer les foncti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JORFTEXT000048734585</td>\n",
       "      <td>JORFARTI000048734586</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>II. — L'article L. 2122-19-1 du code général d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID texte            ID article  Nature N° article  N° alinéa  \\\n",
       "0  JORFTEXT000048734585                   NaN       0        NaN          0   \n",
       "1  JORFTEXT000048734585  JORFVERS000048734585       0        NaN          0   \n",
       "2  JORFTEXT000048734585  JORFARTI000048734586       1          1          1   \n",
       "3  JORFTEXT000048734585  JORFARTI000048734586       1          1          2   \n",
       "4  JORFTEXT000048734585  JORFARTI000048734586       1          1          3   \n",
       "\n",
       "                                             Contenu  \n",
       "0                     fr/lr/loi/2023-1380/2023-12-31  \n",
       "1  LOI n° 2023-1380 du 30 décembre 2023 visant à ...  \n",
       "2  I. — Après l'article L. 2122-19 du code généra...  \n",
       "3  « Art. L. 2122-19-1. — Pour assurer les foncti...  \n",
       "4  II. — L'article L. 2122-19-1 du code général d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./input_data/jorf_2023.csv\",sep='|',names=[\"ID texte\",\"ID article\",\"Nature\",\"N° article\",\"N° alinéa\",\"Contenu\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de caractères :  8846675\n",
      "Nombre de caractères :  52633868\n"
     ]
    }
   ],
   "source": [
    "# Ecrire l'ensemble du contenu dans un fichier texte\n",
    "with open(\"input_data/jorf_2023.txt\",\"w\") as f:\n",
    "    f.writelines(\"\\n\".join(df[\"Contenu\"].to_list()))\n",
    "    \n",
    "# On extrait que le Droit : contenus au sein de guillemets\n",
    "with open(\"./input_data/jorf_2023_droit.txt\",\"w\") as f:\n",
    "    df2 = df[df[\"Contenu\"].str[0] == \"«\"]\n",
    "    data = \"\\n\".join(df2[\"Contenu\"].to_list())\n",
    "    print(\"Nombre de caractères : \", len(data))\n",
    "    f.writelines(data)\n",
    "    \n",
    "# On extrait que ce qui entoure le Droit\n",
    "with open(\"./input_data/jorf_2023_non_droit.txt\",\"w\") as f:\n",
    "    df2 = df[~(df[\"Contenu\"].str[0] == \"«\")]\n",
    "    data = \"\\n\".join(df2[\"Contenu\"].to_list())\n",
    "    print(\"Nombre de caractères : \", len(data))\n",
    "    f.writelines(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entraînement des modèles\n",
    "\n",
    "### 2.A. Test sur le nombre de tokens autorisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./jorf_2023.txt\n",
      "  input_format: \n",
      "  model_prefix: vs100\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(352) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(184) LOG(INFO) Loading corpus: ./jorf_2023.txt\n",
      "trainer_interface.cc(379) LOG(WARNING) Found too long line (5374 > 4192).\n",
      "trainer_interface.cc(381) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(382) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(408) LOG(INFO) Loaded all 454328 sentences\n",
      "trainer_interface.cc(415) LOG(INFO) Skipped 12 too long sentences.\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(429) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(538) LOG(INFO) all chars count=61397590\n",
      "trainer_interface.cc(549) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(559) LOG(INFO) Alphabet size=93\n",
      "trainer_interface.cc(560) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 454328 sentences.\n",
      "unigram_model_trainer.cc(223) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(227) LOG(INFO) Extracting frequent sub strings... node_num=41366633\n",
      "unigram_model_trainer.cc(275) LOG(INFO) Initialized 383472 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 454328\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 285244\n",
      "unigram_model_trainer.cc(565) LOG(INFO) Using 285244 sentences for EM training\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=160772 obj=12.0076 num_tokens=874807 num_tokens/piece=5.44129\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=138836 obj=9.14957 num_tokens=875883 num_tokens/piece=6.30876\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=104096 obj=9.13354 num_tokens=904911 num_tokens/piece=8.69304\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=103993 obj=9.12275 num_tokens=906005 num_tokens/piece=8.71217\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=77992 obj=9.15531 num_tokens=944293 num_tokens/piece=12.1076\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=77991 obj=9.14688 num_tokens=944219 num_tokens/piece=12.1068\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=58493 obj=9.1929 num_tokens=986985 num_tokens/piece=16.8736\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=58493 obj=9.18221 num_tokens=986882 num_tokens/piece=16.8718\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=43868 obj=9.24205 num_tokens=1031095 num_tokens/piece=23.5045\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=43868 obj=9.23014 num_tokens=1030966 num_tokens/piece=23.5016\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=32901 obj=9.3063 num_tokens=1078340 num_tokens/piece=32.7753\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=32901 obj=9.29267 num_tokens=1078200 num_tokens/piece=32.771\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=24675 obj=9.3916 num_tokens=1127237 num_tokens/piece=45.6834\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=24675 obj=9.37384 num_tokens=1127176 num_tokens/piece=45.6809\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=18506 obj=9.49856 num_tokens=1180502 num_tokens/piece=63.7902\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=18506 obj=9.4756 num_tokens=1180558 num_tokens/piece=63.7933\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=13879 obj=9.6363 num_tokens=1243857 num_tokens/piece=89.6215\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=13879 obj=9.6057 num_tokens=1243995 num_tokens/piece=89.6315\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=10409 obj=9.80444 num_tokens=1302401 num_tokens/piece=125.123\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=10409 obj=9.76579 num_tokens=1302621 num_tokens/piece=125.144\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=7806 obj=10.0194 num_tokens=1368821 num_tokens/piece=175.355\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=7806 obj=9.96906 num_tokens=1368966 num_tokens/piece=175.374\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=5854 obj=10.2778 num_tokens=1432354 num_tokens/piece=244.68\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=5854 obj=10.2197 num_tokens=1432448 num_tokens/piece=244.696\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=4390 obj=10.5865 num_tokens=1497777 num_tokens/piece=341.179\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=4390 obj=10.5123 num_tokens=1498003 num_tokens/piece=341.231\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=3292 obj=10.9479 num_tokens=1570201 num_tokens/piece=476.975\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=3292 obj=10.8636 num_tokens=1570365 num_tokens/piece=477.025\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=2469 obj=11.3785 num_tokens=1640642 num_tokens/piece=664.497\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=2469 obj=11.2869 num_tokens=1640763 num_tokens/piece=664.546\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1851 obj=11.9118 num_tokens=1743081 num_tokens/piece=941.697\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1851 obj=11.7956 num_tokens=1742994 num_tokens/piece=941.65\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1388 obj=12.4548 num_tokens=1885919 num_tokens/piece=1358.73\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1388 obj=12.3313 num_tokens=1885960 num_tokens/piece=1358.76\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1041 obj=13.069 num_tokens=2005034 num_tokens/piece=1926.07\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1041 obj=12.9263 num_tokens=2005130 num_tokens/piece=1926.16\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=780 obj=13.695 num_tokens=2094529 num_tokens/piece=2685.29\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=780 obj=13.5378 num_tokens=2094433 num_tokens/piece=2685.17\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=585 obj=14.3787 num_tokens=2253012 num_tokens/piece=3851.3\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=585 obj=14.2069 num_tokens=2253009 num_tokens/piece=3851.3\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=438 obj=15.1211 num_tokens=2369987 num_tokens/piece=5410.93\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=438 obj=14.9323 num_tokens=2369913 num_tokens/piece=5410.76\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=328 obj=15.9782 num_tokens=2477905 num_tokens/piece=7554.59\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=328 obj=15.737 num_tokens=2477991 num_tokens/piece=7554.85\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=246 obj=16.9948 num_tokens=2592998 num_tokens/piece=10540.6\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=246 obj=16.6813 num_tokens=2592998 num_tokens/piece=10540.6\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=184 obj=18.0263 num_tokens=2748091 num_tokens/piece=14935.3\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=184 obj=17.4822 num_tokens=2748091 num_tokens/piece=14935.3\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=138 obj=19.0502 num_tokens=2877635 num_tokens/piece=20852.4\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=138 obj=18.4877 num_tokens=2877636 num_tokens/piece=20852.4\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=110 obj=20.1731 num_tokens=3016708 num_tokens/piece=27424.6\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=110 obj=19.5131 num_tokens=3016708 num_tokens/piece=27424.6\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: vs100.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: vs100.vocab\n"
     ]
    }
   ],
   "source": [
    "for n in (100,1000,10000):\n",
    "    spm.SentencePieceTrainer.train(input='./input_data/jorf_2023.txt', model_prefix='./models/vs{n}', vocab_size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding with 100 tokens :\n",
      "['▁', 'U', 'n', '▁d', 'é', 'c', 're', 't', '▁', 'e', 'n', '▁', 'C', 'o', 'n', 's', 'e', 'i', 'l', '▁d', \"'\", 'E', 't', 'a', 't', '▁', 'p', 'r', 'é', 'c', 'i', 's', 'e', '▁', 'l', 'es', '▁', 'm', 'o', 'd', 'a', 'l', 'i', 't', 'é', 's', '▁d', \"'\", 'a', 'p', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n', '▁d', 'u', '▁', 'p', 'r', 'é', 's', 'e', 'n', 't', '▁', 'a', 'r', 't', 'i', 'c', 'l', 'e']\n",
      "Encoding with 1000 tokens :\n",
      "['▁Un', '▁décret', '▁en', '▁Conseil', '▁d', \"'\", 'Etat', '▁précise', '▁les', '▁modalités', '▁d', \"'\", 'application', '▁du', '▁présent', '▁', 'article']\n",
      "Encoding with 10000 tokens :\n",
      "['▁Un', '▁décret', '▁en', '▁Conseil', '▁d', \"'\", 'Etat', '▁précise', '▁les', '▁modalités', '▁d', \"'\", 'application', '▁du', '▁présent', '▁article']\n"
     ]
    }
   ],
   "source": [
    "# On encode une phrase selon les différents modèles : 100 tokens appara^t clairement insuffisant\n",
    "for n in [100,1000,10000]:\n",
    "    sp = spm.SentencePieceProcessor(model_file=f'./models/vs{n}.model')\n",
    "    print(f\"Encoding with {n} tokens :\")\n",
    "    print(sp.encode(\"Un décret en Conseil d'Etat précise les modalités d'application du présent article\",out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B. Test de l'effet de n'inclure que du Droit / que des auxilliaires au Droit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./input_data/jorf_2023_droit.txt\n",
      "  input_format: \n",
      "  model_prefix: ./models/subset_droit\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(352) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(184) LOG(INFO) Loading corpus: ./input_data/jorf_2023_droit.txt\n",
      "trainer_interface.cc(408) LOG(INFO) Loaded all 45509 sentences\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(429) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(538) LOG(INFO) all chars count=8846802\n",
      "trainer_interface.cc(549) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(559) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(560) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 45509 sentences.\n",
      "unigram_model_trainer.cc(223) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(227) LOG(INFO) Extracting frequent sub strings... node_num=5917429\n",
      "unigram_model_trainer.cc(275) LOG(INFO) Initialized 90134 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 45509\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 52790\n",
      "unigram_model_trainer.cc(565) LOG(INFO) Using 52790 sentences for EM training\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=33846 obj=11.1843 num_tokens=110657 num_tokens/piece=3.26943\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=27705 obj=8.46472 num_tokens=110939 num_tokens/piece=4.0043\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=20773 obj=8.45906 num_tokens=117681 num_tokens/piece=5.66509\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=20765 obj=8.44478 num_tokens=117678 num_tokens/piece=5.66713\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=15572 obj=8.51912 num_tokens=128570 num_tokens/piece=8.25649\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=15571 obj=8.50042 num_tokens=128546 num_tokens/piece=8.25547\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=11678 obj=8.61941 num_tokens=141407 num_tokens/piece=12.1088\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=11678 obj=8.59619 num_tokens=141400 num_tokens/piece=12.1082\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=8758 obj=8.76355 num_tokens=155621 num_tokens/piece=17.769\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=8758 obj=8.73226 num_tokens=155609 num_tokens/piece=17.7676\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=6568 obj=8.95119 num_tokens=170896 num_tokens/piece=26.0195\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=6568 obj=8.90696 num_tokens=170909 num_tokens/piece=26.0215\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=4926 obj=9.18354 num_tokens=186724 num_tokens/piece=37.9058\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=4926 obj=9.13168 num_tokens=186780 num_tokens/piece=37.9172\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=3694 obj=9.47627 num_tokens=203708 num_tokens/piece=55.1456\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=3694 obj=9.41236 num_tokens=204316 num_tokens/piece=55.3102\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=2770 obj=9.85492 num_tokens=222103 num_tokens/piece=80.1816\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=2770 obj=9.77586 num_tokens=222152 num_tokens/piece=80.1993\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=2077 obj=10.3045 num_tokens=239339 num_tokens/piece=115.233\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=2077 obj=10.2177 num_tokens=239349 num_tokens/piece=115.238\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1557 obj=10.7956 num_tokens=257200 num_tokens/piece=165.189\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1557 obj=10.6992 num_tokens=257203 num_tokens/piece=165.191\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1167 obj=11.3699 num_tokens=271633 num_tokens/piece=232.762\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1167 obj=11.272 num_tokens=271677 num_tokens/piece=232.799\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1100 obj=11.4093 num_tokens=274801 num_tokens/piece=249.819\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1100 obj=11.3933 num_tokens=274811 num_tokens/piece=249.828\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./models/subset_droit.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./models/subset_droit.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./input_data/jorf_2023_non_droit.txt\n",
      "  input_format: \n",
      "  model_prefix: ./models/subset_non_droit\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(352) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(184) LOG(INFO) Loading corpus: ./input_data/jorf_2023_non_droit.txt\n",
      "trainer_interface.cc(379) LOG(WARNING) Found too long line (5374 > 4192).\n",
      "trainer_interface.cc(381) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(382) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(408) LOG(INFO) Loaded all 408819 sentences\n",
      "trainer_interface.cc(415) LOG(INFO) Skipped 12 too long sentences.\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(424) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(429) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(538) LOG(INFO) all chars count=52550788\n",
      "trainer_interface.cc(549) LOG(INFO) Done: 99.9557% characters are covered.\n",
      "trainer_interface.cc(559) LOG(INFO) Alphabet size=93\n",
      "trainer_interface.cc(560) LOG(INFO) Final character coverage=0.999557\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 408819 sentences.\n",
      "unigram_model_trainer.cc(223) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(227) LOG(INFO) Extracting frequent sub strings... node_num=35389201\n",
      "unigram_model_trainer.cc(275) LOG(INFO) Initialized 361097 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 408819\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 267884\n",
      "unigram_model_trainer.cc(565) LOG(INFO) Using 267884 sentences for EM training\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=152314 obj=12.1184 num_tokens=839958 num_tokens/piece=5.51465\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=131602 obj=9.25767 num_tokens=841478 num_tokens/piece=6.39411\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=98677 obj=9.24079 num_tokens=868816 num_tokens/piece=8.80465\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=98568 obj=9.22887 num_tokens=869592 num_tokens/piece=8.82225\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=73925 obj=9.26356 num_tokens=905087 num_tokens/piece=12.2433\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=73922 obj=9.25469 num_tokens=904986 num_tokens/piece=12.2424\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=55441 obj=9.30402 num_tokens=945176 num_tokens/piece=17.0483\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=55441 obj=9.29234 num_tokens=945031 num_tokens/piece=17.0457\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=41580 obj=9.35595 num_tokens=986196 num_tokens/piece=23.718\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=41580 obj=9.343 num_tokens=986057 num_tokens/piece=23.7147\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=31185 obj=9.42377 num_tokens=1030306 num_tokens/piece=33.0385\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=31185 obj=9.40922 num_tokens=1030216 num_tokens/piece=33.0356\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=23388 obj=9.51316 num_tokens=1076477 num_tokens/piece=46.0269\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=23388 obj=9.49546 num_tokens=1076449 num_tokens/piece=46.0257\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=17541 obj=9.63293 num_tokens=1126350 num_tokens/piece=64.2124\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=17541 obj=9.60822 num_tokens=1126367 num_tokens/piece=64.2134\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=13155 obj=9.77774 num_tokens=1183105 num_tokens/piece=89.9358\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=13154 obj=9.75651 num_tokens=1183268 num_tokens/piece=89.955\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=9865 obj=9.95572 num_tokens=1241167 num_tokens/piece=125.815\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=9865 obj=9.91474 num_tokens=1241241 num_tokens/piece=125.823\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=7398 obj=10.1776 num_tokens=1304603 num_tokens/piece=176.345\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=7398 obj=10.1254 num_tokens=1304703 num_tokens/piece=176.359\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=5548 obj=10.4406 num_tokens=1364796 num_tokens/piece=245.998\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=5548 obj=10.3785 num_tokens=1364973 num_tokens/piece=246.03\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=4161 obj=10.7589 num_tokens=1427509 num_tokens/piece=343.069\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=4161 obj=10.6855 num_tokens=1430080 num_tokens/piece=343.687\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=3120 obj=11.145 num_tokens=1507198 num_tokens/piece=483.076\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=3120 obj=11.0542 num_tokens=1507311 num_tokens/piece=483.113\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=2340 obj=11.5854 num_tokens=1574563 num_tokens/piece=672.89\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=2340 obj=11.4853 num_tokens=1574675 num_tokens/piece=672.938\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1755 obj=12.0978 num_tokens=1666916 num_tokens/piece=949.81\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1755 obj=11.9783 num_tokens=1666961 num_tokens/piece=949.835\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1316 obj=12.6542 num_tokens=1786123 num_tokens/piece=1357.24\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1316 obj=12.5246 num_tokens=1786209 num_tokens/piece=1357.3\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=0 size=1100 obj=12.9478 num_tokens=1857509 num_tokens/piece=1688.64\n",
      "unigram_model_trainer.cc(581) LOG(INFO) EM sub_iter=1 size=1100 obj=12.875 num_tokens=1857542 num_tokens/piece=1688.67\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./models/subset_non_droit.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./models/subset_non_droit.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input='./input_data/jorf_2023_droit.txt', model_prefix='./models/subset_droit', vocab_size=1000)\n",
    "spm.SentencePieceTrainer.train(input='./input_data/jorf_2023_non_droit.txt', model_prefix='./models/subset_non_droit', vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = spm.SentencePieceProcessor(model_file=f'./models/subset_droit.model')\n",
    "sp2 = spm.SentencePieceProcessor(model_file=f'./models/subset_non_droit.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "Tokenisation de la phrase : « Les modalités de calcul du montant de la dotation complémentaire prise en application des présents II et III, sont définies à l'annexe 4. »\n",
      "['▁«', '▁Les', '▁modalités', '▁de', '▁calcul', '▁du', '▁montant', '▁de', '▁la', '▁d', 'ot', 'ation', '▁complémentaire', '▁prise', '▁en', '▁application', '▁des', '▁présent', 's', '▁II', '▁et', '▁III', ',', '▁sont', '▁défini', 'es', '▁à', '▁l', \"'\", 'annexe', '▁4', '.', '▁»']\n",
      "['▁«', '▁Les', '▁modalités', '▁de', '▁calcul', '▁du', '▁montant', '▁de', '▁la', '▁d', 'ot', 'ation', '▁complémentaire', '▁p', 'ri', 's', 'e', '▁en', '▁', 'application', '▁des', '▁présent', 's', '▁II', '▁et', '▁III', ',', '▁sont', '▁défini', 'es', '▁à', '▁l', \"'\", 'annexe', '▁4', '.', '▁»']\n",
      "#####\n",
      "Tokenisation de la phrase : Arrêté du 22 décembre 2022 portant nomination au conseil d'administration de l'établissement public d'aménagement de Bordeaux-Euratlantique\n",
      "['▁A', 'r', 'r', 'ê', 'té', '▁du', '▁22', '▁décembre', '▁2022', '▁portant', '▁n', 'om', 'in', 'ation', '▁au', '▁conseil', '▁d', \"'\", 'administration', '▁de', '▁l', \"'\", 'établissement', '▁public', '▁d', \"'\", 'aménagement', '▁de', '▁B', 'or', 'de', 'aux', '-', 'E', 'ur', 'at', 'l', 'ant', 'ique']\n",
      "['▁Arrêté', '▁du', '▁22', '▁décembre', '▁2022', '▁portant', '▁nomination', '▁au', '▁conseil', '▁d', \"'\", 'administration', '▁de', '▁l', \"'\", 'établissement', '▁public', '▁d', \"'\", 'aménagement', '▁de', '▁B', 'ord', 'e', 'aux', '-', 'E', 'ur', 'at', 'l', 'ant', 'ique']\n"
     ]
    }
   ],
   "source": [
    "# Comparaison de la tokenisation d'une phrase de droit et d'une non de Droit \n",
    "s1 = \"« Les modalités de calcul du montant de la dotation complémentaire prise en application des présents II et III, sont définies à l'annexe 4. »\"\n",
    "s2 = \"Arrêté du 22 décembre 2022 portant nomination au conseil d'administration de l'établissement public d'aménagement de Bordeaux-Euratlantique\"\n",
    "\n",
    "for s in [s1,s2]:\n",
    "    print(\"#####\")\n",
    "    print(\"Tokenisation de la phrase : \" + s)\n",
    "    for sp in [sp1,sp2]:\n",
    "        print(sp.encode(s,out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe des différences notables. Par exemple le mot \"Arrêté\" n'est pas un token du modèle entraîné uniquement sur du Droit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
