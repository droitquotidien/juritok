{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jorf 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sentencepiece as spm \n",
    "\n",
    "jorf2023 = \"/Users/tifftanya/Documents/Les Mines/2023-2024/MSI/juritok/jorf_2023.csv\"\n",
    "column_names = ['Text ID', 'Article ID', 'Type of text', 'Part 1', 'Part 2', 'Text']\n",
    "df2023 = pd.read_csv(jorf2023, names = column_names, sep='|')\n",
    "\n",
    "text2023 = df2023.iloc[:, -1]\n",
    "\n",
    "text_2023 = 'text2023.txt'\n",
    "text2023.to_csv(text_2023, header = False, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens for model model_100.model:\n",
      "['▁', 'f', 'r', '/', 'l', 'r', '/', 'l', 'o', 'i', '/', '2', '0', '2', '3', '-', '1', '3', '8', '0', '/', '2', '0', '2', '3', '-', '1', '2', '-', '3', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input_format=text --input=text2023.txt --model_prefix=model_100 --vocab_size=100\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text2023.txt\n",
      "  input_format: text\n",
      "  model_prefix: model_100\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: text2023.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5376 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 454328 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 12 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=61803752\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=93\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 454328 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=41708741\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 381615 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 454328\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 305358\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 305358 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=164030 obj=12.2926 num_tokens=953184 num_tokens/piece=5.81103\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=142677 obj=9.37609 num_tokens=954755 num_tokens/piece=6.69172\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=106982 obj=9.35817 num_tokens=985677 num_tokens/piece=9.21348\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=106893 obj=9.34561 num_tokens=987236 num_tokens/piece=9.23574\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=80167 obj=9.37899 num_tokens=1026493 num_tokens/piece=12.8044\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=80163 obj=9.37015 num_tokens=1026839 num_tokens/piece=12.8094\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=60122 obj=9.41711 num_tokens=1070741 num_tokens/piece=17.8095\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=60122 obj=9.406 num_tokens=1070573 num_tokens/piece=17.8067\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=45091 obj=9.46615 num_tokens=1115901 num_tokens/piece=24.7478\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=45091 obj=9.45396 num_tokens=1115710 num_tokens/piece=24.7435\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33818 obj=9.53197 num_tokens=1165405 num_tokens/piece=34.4611\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33818 obj=9.51775 num_tokens=1165285 num_tokens/piece=34.4575\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=25363 obj=9.61691 num_tokens=1217549 num_tokens/piece=48.0049\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25363 obj=9.59903 num_tokens=1217434 num_tokens/piece=48.0004\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19022 obj=9.71895 num_tokens=1275639 num_tokens/piece=67.0612\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19022 obj=9.69608 num_tokens=1275706 num_tokens/piece=67.0648\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14266 obj=9.85431 num_tokens=1341642 num_tokens/piece=94.0447\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14266 obj=9.82398 num_tokens=1341719 num_tokens/piece=94.0501\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10699 obj=10.02 num_tokens=1405925 num_tokens/piece=131.407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10699 obj=9.98207 num_tokens=1406275 num_tokens/piece=131.44\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8024 obj=10.2343 num_tokens=1470416 num_tokens/piece=183.252\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8024 obj=10.1847 num_tokens=1470534 num_tokens/piece=183.267\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6018 obj=10.4879 num_tokens=1538256 num_tokens/piece=255.609\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6018 obj=10.4305 num_tokens=1538346 num_tokens/piece=255.624\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4513 obj=10.792 num_tokens=1608094 num_tokens/piece=356.325\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4513 obj=10.7207 num_tokens=1610716 num_tokens/piece=356.906\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3384 obj=11.1479 num_tokens=1691895 num_tokens/piece=499.969\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3384 obj=11.0641 num_tokens=1691981 num_tokens/piece=499.994\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2538 obj=11.584 num_tokens=1769987 num_tokens/piece=697.394\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2538 obj=11.4905 num_tokens=1770177 num_tokens/piece=697.469\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1903 obj=12.0967 num_tokens=1877090 num_tokens/piece=986.385\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1903 obj=11.9826 num_tokens=1877028 num_tokens/piece=986.352\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1427 obj=12.6502 num_tokens=2022779 num_tokens/piece=1417.5\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1427 obj=12.5188 num_tokens=2022853 num_tokens/piece=1417.56\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1070 obj=13.2289 num_tokens=2160766 num_tokens/piece=2019.41\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1070 obj=13.0976 num_tokens=2160831 num_tokens/piece=2019.47\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=802 obj=13.8773 num_tokens=2250329 num_tokens/piece=2805.9\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=802 obj=13.7194 num_tokens=2250534 num_tokens/piece=2806.15\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=601 obj=14.5894 num_tokens=2368753 num_tokens/piece=3941.35\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=601 obj=14.4151 num_tokens=2368759 num_tokens/piece=3941.36\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=450 obj=15.3332 num_tokens=2510370 num_tokens/piece=5578.6\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=450 obj=15.1231 num_tokens=2510411 num_tokens/piece=5578.69\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=337 obj=16.1289 num_tokens=2598332 num_tokens/piece=7710.18\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=337 obj=15.9384 num_tokens=2598354 num_tokens/piece=7710.25\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=252 obj=17.1385 num_tokens=2748562 num_tokens/piece=10907\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=252 obj=16.8205 num_tokens=2748729 num_tokens/piece=10907.7\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=189 obj=18.091 num_tokens=2908507 num_tokens/piece=15388.9\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=189 obj=17.727 num_tokens=2908507 num_tokens/piece=15388.9\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=141 obj=19.3794 num_tokens=3082179 num_tokens/piece=21859.4\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=141 obj=18.6473 num_tokens=3082179 num_tokens/piece=21859.4\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=110 obj=20.5013 num_tokens=3226205 num_tokens/piece=29329.1\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=110 obj=19.7517 num_tokens=3226205 num_tokens/piece=29329.1\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: model_100.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: model_100.vocab\n"
     ]
    }
   ],
   "source": [
    "with open('text2023.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "    sample_sentence = text_data.splitlines()[0].strip()\n",
    "\n",
    "model_prefix_100 = 'model_100'\n",
    "training_command = f\"--input_format=text --input=text2023.txt --model_prefix={model_prefix_100} --vocab_size=100\"\n",
    "spm.SentencePieceTrainer.Train(training_command)\n",
    "\n",
    "sp_100 = spm.SentencePieceProcessor()\n",
    "sp_100.load(f\"{model_prefix_100}.model\")\n",
    "tokens_100 = sp_100.EncodeAsPieces(sample_sentence)\n",
    "\n",
    "print(f\"\\nTokens for model {model_prefix_100}.model:\")\n",
    "print(tokens_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens for model model_10000.model:\n",
      "['▁\"', 'Par', '▁décret', '▁du', '▁Président', '▁de', '▁la', '▁République', '▁en', '▁date', '▁du', '▁29', '▁décembre', '▁2023,', '▁pris', '▁sur', '▁le', '▁rapport', '▁de', '▁la', '▁P', 'remière', '▁ministre', '▁et', '▁visé', '▁pour', '▁son', '▁', 'exécution', '▁par', '▁le', '▁grand', '▁chancelier', '▁de', '▁la', '▁Légion', '▁d', \"'\", 'honneur', ',', '▁vu', '▁la', '▁déclaration', '▁du', '▁conseil', '▁de', '▁l', \"'\", 'ordre', '▁portant', '▁que', '▁les', '▁présente', 's', '▁élévation', 's', '▁sont', '▁faites', '▁en', '▁conformité', '▁des', '▁loi', 's', ',', '▁décrets', '▁et', '▁règlements', '▁en', '▁vigueur', ',', '▁le', '▁conseil', '▁des', '▁ministres', '▁entendu', ',', '▁sont', '▁élevé', 's', ',', '▁pour', '▁prendre', '▁rang', '▁à', '▁compter', '▁de', '▁la', '▁date', '▁de', '▁réception', '▁dans', '▁leur', '▁dignité', '▁:', '\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input_format=text --input=text2023.txt --model_prefix=model_10000 --vocab_size=10000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text2023.txt\n",
      "  input_format: text\n",
      "  model_prefix: model_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: text2023.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5376 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 454328 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 12 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=61803752\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=93\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 454328 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=41708741\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 381615 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 454328\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 305358\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 305358 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=164030 obj=12.2926 num_tokens=953184 num_tokens/piece=5.81103\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=142677 obj=9.37609 num_tokens=954755 num_tokens/piece=6.69172\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=106982 obj=9.35817 num_tokens=985677 num_tokens/piece=9.21348\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=106893 obj=9.34561 num_tokens=987236 num_tokens/piece=9.23574\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=80167 obj=9.37899 num_tokens=1026493 num_tokens/piece=12.8044\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=80163 obj=9.37015 num_tokens=1026839 num_tokens/piece=12.8094\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=60122 obj=9.41711 num_tokens=1070741 num_tokens/piece=17.8095\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=60122 obj=9.406 num_tokens=1070573 num_tokens/piece=17.8067\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=45091 obj=9.46615 num_tokens=1115901 num_tokens/piece=24.7478\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=45091 obj=9.45396 num_tokens=1115710 num_tokens/piece=24.7435\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33818 obj=9.53197 num_tokens=1165405 num_tokens/piece=34.4611\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33818 obj=9.51775 num_tokens=1165285 num_tokens/piece=34.4575\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=25363 obj=9.61691 num_tokens=1217549 num_tokens/piece=48.0049\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25363 obj=9.59903 num_tokens=1217434 num_tokens/piece=48.0004\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19022 obj=9.71895 num_tokens=1275639 num_tokens/piece=67.0612\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19022 obj=9.69608 num_tokens=1275706 num_tokens/piece=67.0648\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14266 obj=9.85431 num_tokens=1341642 num_tokens/piece=94.0447\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14266 obj=9.82398 num_tokens=1341719 num_tokens/piece=94.0501\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11000 obj=9.99721 num_tokens=1400268 num_tokens/piece=127.297\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11000 obj=9.96397 num_tokens=1400593 num_tokens/piece=127.327\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: model_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: model_10000.vocab\n"
     ]
    }
   ],
   "source": [
    "with open('text2023.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "    sample_sentence = text_data.splitlines()[28].strip()\n",
    "\n",
    "model_prefix_10000 = 'model_10000'\n",
    "training_command = f\"--input_format=text --input=text2023.txt --model_prefix={model_prefix_10000} --vocab_size=10000\"\n",
    "spm.SentencePieceTrainer.Train(training_command)\n",
    "\n",
    "sp_10000 = spm.SentencePieceProcessor()\n",
    "sp_10000.load(f\"{model_prefix_10000}.model\")\n",
    "tokens_10000 = sp_10000.EncodeAsPieces(sample_sentence)\n",
    "\n",
    "print(f\"\\nTokens for model {model_prefix_10000}.model:\")\n",
    "print(tokens_10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jorf 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jorf2022 = \"/Users/tifftanya/Documents/Les Mines/2023-2024/MSI/juritok/jorf_2022.csv\"\n",
    "column_names = ['Text ID', 'Article ID', 'Type of text', 'Part 1', 'Part 2', 'Text']\n",
    "df2022 = pd.read_csv(jorf2022, names = column_names, sep='|')\n",
    "\n",
    "text2022 = df2022.iloc[:, -1]\n",
    "\n",
    "text_2022 = 'text2022.txt'\n",
    "text2022.to_csv(text_2022, header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens for model model_15000.model:\n",
      "['▁\"', '—', '▁à', '▁la', '▁fin', '▁du', '▁troisième', '▁alinéa', '▁et', '▁à', '▁l', \"'\", 'avant', '-', 'dernier', '▁alinéa', ',', '▁le', '▁montant', '▁:', '▁«', '▁74', '▁5', '45', '▁', '€', '▁»', '▁est', '▁remplacé', '▁par', '▁le', '▁montant', '▁:', '▁«', '▁78', '▁5', '70', '▁', '€', '▁»', '▁;', '\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input_format=text --input=text2022.txt --model_prefix=model_15000 --vocab_size=15000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text2022.txt\n",
      "  input_format: text\n",
      "  model_prefix: model_15000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 15000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: text2022.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (7582 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 459296 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 14 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=63451032\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9537% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=93\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999537\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 459295 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=43021169\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 373757 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 459295\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 300191\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 300191 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=160166 obj=12.1793 num_tokens=954221 num_tokens/piece=5.9577\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=139402 obj=9.2857 num_tokens=955634 num_tokens/piece=6.85524\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=104534 obj=9.26733 num_tokens=984808 num_tokens/piece=9.42093\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=104448 obj=9.25629 num_tokens=985483 num_tokens/piece=9.43515\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=78332 obj=9.28781 num_tokens=1023136 num_tokens/piece=13.0615\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=78325 obj=9.28003 num_tokens=1023043 num_tokens/piece=13.0615\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=58743 obj=9.32398 num_tokens=1065344 num_tokens/piece=18.1357\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=58742 obj=9.31352 num_tokens=1065175 num_tokens/piece=18.1331\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=44056 obj=9.37114 num_tokens=1109042 num_tokens/piece=25.1735\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=44055 obj=9.36015 num_tokens=1108896 num_tokens/piece=25.1707\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33040 obj=9.43462 num_tokens=1157897 num_tokens/piece=35.0453\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33040 obj=9.42082 num_tokens=1157808 num_tokens/piece=35.0426\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=24780 obj=9.51518 num_tokens=1209282 num_tokens/piece=48.8007\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=24780 obj=9.49806 num_tokens=1209311 num_tokens/piece=48.8019\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=18585 obj=9.61067 num_tokens=1264348 num_tokens/piece=68.0306\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=18585 obj=9.58929 num_tokens=1264362 num_tokens/piece=68.0313\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16500 obj=9.64343 num_tokens=1288549 num_tokens/piece=78.0939\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16500 obj=9.6337 num_tokens=1288619 num_tokens/piece=78.0981\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: model_15000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: model_15000.vocab\n"
     ]
    }
   ],
   "source": [
    "with open('text2022.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "    sample_sentence = text_data.splitlines()[19].strip()\n",
    "\n",
    "model_prefix_15000 = 'model_15000'\n",
    "training_command = f\"--input_format=text --input=text2022.txt --model_prefix={model_prefix_15000} --vocab_size=15000\"\n",
    "spm.SentencePieceTrainer.Train(training_command)\n",
    "\n",
    "sp_15000 = spm.SentencePieceProcessor()\n",
    "sp_15000.load(f\"{model_prefix_15000}.model\")\n",
    "tokens_15000 = sp_15000.EncodeAsPieces(sample_sentence)\n",
    "\n",
    "print(f\"\\nTokens for model {model_prefix_15000}.model:\")\n",
    "print(tokens_15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jorf 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jorf2021 = \"/Users/tifftanya/Documents/Les Mines/2023-2024/MSI/juritok/jorf_2021.csv\"\n",
    "column_names = ['Text ID', 'Article ID', 'Type of text', 'Part 1', 'Part 2', 'Text']\n",
    "df2021 = pd.read_csv(jorf2021, names = column_names, sep='|')\n",
    "\n",
    "text2021 = df2021.iloc[:, -1]\n",
    "\n",
    "text_2021 = 'text2021.txt'\n",
    "text2021.to_csv(text_2021, header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens for model model_16000.model:\n",
      "['▁2°', '▁Le', '▁I', '▁de', '▁l', \"'\", 'article', '▁19', '7', '▁est', '▁ainsi', '▁modifié', '▁:']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input_format=text --input=text2021.txt --model_prefix=model_16000 --vocab_size=16000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: text2021.txt\n",
      "  input_format: text\n",
      "  model_prefix: model_16000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: text2021.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4889 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 482140 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 12 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=68756015\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9514% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=92\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999514\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 482140 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=46242497\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 421782 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 482140\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 335551\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 335551 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=178601 obj=12.1393 num_tokens=1043932 num_tokens/piece=5.84505\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=154494 obj=9.28304 num_tokens=1045326 num_tokens/piece=6.76613\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=115844 obj=9.26438 num_tokens=1077294 num_tokens/piece=9.29952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=115753 obj=9.25375 num_tokens=1078056 num_tokens/piece=9.31342\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=86811 obj=9.28759 num_tokens=1122148 num_tokens/piece=12.9263\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=86796 obj=9.27902 num_tokens=1122012 num_tokens/piece=12.927\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65095 obj=9.32522 num_tokens=1169410 num_tokens/piece=17.9647\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65095 obj=9.315 num_tokens=1169188 num_tokens/piece=17.9613\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=48821 obj=9.36928 num_tokens=1217187 num_tokens/piece=24.9316\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=48821 obj=9.35767 num_tokens=1217065 num_tokens/piece=24.9291\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=36615 obj=9.43279 num_tokens=1269813 num_tokens/piece=34.6801\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=36615 obj=9.41957 num_tokens=1269663 num_tokens/piece=34.676\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27461 obj=9.51306 num_tokens=1326648 num_tokens/piece=48.3103\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27461 obj=9.49654 num_tokens=1326604 num_tokens/piece=48.3087\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20595 obj=9.60777 num_tokens=1390951 num_tokens/piece=67.5383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20592 obj=9.59296 num_tokens=1391155 num_tokens/piece=67.558\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17600 obj=9.65445 num_tokens=1428969 num_tokens/piece=81.1914\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17600 obj=9.64224 num_tokens=1429090 num_tokens/piece=81.1983\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: model_16000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: model_16000.vocab\n"
     ]
    }
   ],
   "source": [
    "with open('text2021.txt', 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "    sample_sentence = text_data.splitlines()[15].strip()\n",
    "\n",
    "model_prefix_16000 = 'model_16000'\n",
    "training_command = f\"--input_format=text --input=text2021.txt --model_prefix={model_prefix_16000} --vocab_size=16000\"\n",
    "spm.SentencePieceTrainer.Train(training_command)\n",
    "\n",
    "sp_16000 = spm.SentencePieceProcessor()\n",
    "sp_16000.load(f\"{model_prefix_16000}.model\")\n",
    "tokens_16000 = sp_16000.EncodeAsPieces(sample_sentence)\n",
    "\n",
    "print(f\"\\nTokens for model {model_prefix_16000}.model:\")\n",
    "print(tokens_16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "juritok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
