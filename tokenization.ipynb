{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece Tokenizer for Legal Texts Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PIPELINE\n",
    "\n",
    "\tPRE PROCESSING\n",
    "1. Pour chaque fichier CSV:\n",
    "- Tout en minuscule\n",
    "- Suppression caractères spéciales\n",
    "- Suppression des espaces \n",
    "- Suppression des élisions\n",
    "- Stemmisation\n",
    "- Suppression des \"stop words\"\n",
    "\n",
    "\tTRAINING AND TEST CORPUS\n",
    "1. Garder 4/5 fichier CSV pour le modèle de tokenisation\n",
    "2. Tester sur 1 fichier CSV\n",
    "--> Création de fichiers textes représentant le corpus\n",
    "\n",
    "\tANALYSE DU CORPUS POUR DETERMINER LA TAILLE DU VOCABULAIRE\n",
    "1. Comptage des mots\n",
    "2. Décider de la taille du vocabulaire en fonction d'une couverture désirée\n",
    "--> Ici pour couvrir 97%, on choisit une taille du vocabulaire un peu en dessous de 20K mots.\n",
    "\n",
    "\tENTRAINEMENT DU TOKENIZER SENTENCE PIECE\n",
    "\tENCODAGE DU FICHIER TEST\n",
    "\n",
    "\tANALYSE QUANTITATIVE\n",
    "1. Utilisation du vocabulaire (bien environ égal à 97%)\n",
    "2. Longueur moyenne d'un token (si la taille est trop petite cela pourrait indiquer trop de segmentation : 5.67 parait raisonnable\n",
    "3. OOV rate\n",
    "4. Caractéristiques diverses des corpus\n",
    "5. Calcul de l'overlap de vocabulaire entre training et test sets: 64%, ne semble pas suffisant, on devrait augmenter le coverage à 99% voir plus pour bien inclure tous les mots même les rares.\n",
    "\n",
    "\tANALYSE QUALITATIVE\n",
    "Commentaires intégrés au notebook.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies utiles\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import io\n",
    "from collections import Counter\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# On load la librairie spacy pour avoir accès à une liste de stop words \n",
    "import spacy\n",
    "\n",
    "# On utilise cette librairie pour faire une stemmisation avant d'utiliser SentencePiece\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['datas/jorf_2019.csv', 'datas/jorf_2020.csv', 'datas/jorf_2021.csv', 'datas/jorf_2022.csv', 'datas/jorf_2023.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "french_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = FrenchStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # tout en minuscule\n",
    "    text = re.sub(r'\\W+', ' ', text)  # enlèvement des caractères spéciales\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # enlèvement des espaces\n",
    "    # Suppression des élisions\n",
    "    text = re.sub(r\"l'|t'|qu'|n'|s'|c'|j'|d'|m'|jusqu'|lorsqu'|puisqu'\", '', text)\n",
    "\n",
    "    # Suppression des mots vides\n",
    "    words = text.split()\n",
    "    filtered_stemmed_words = [stemmer.stem(word) for word in words if word not in french_stopwords]\n",
    "    \n",
    "    return ' '.join(filtered_stemmed_words)\n",
    "\n",
    "train_texts = []\n",
    "for file in file_paths[:-1]:\n",
    "    df = pd.read_csv(file, sep='|')\n",
    "    df['text'] = df[df.columns[-1]].apply(preprocess_text)\n",
    "    train_texts.extend(df['text'].tolist())\n",
    "\n",
    "test_df = pd.read_csv(file_paths[-1], sep='|')\n",
    "test_df['text'] = test_df[test_df.columns[-1]].apply(preprocess_text)\n",
    "test_texts = []\n",
    "test_texts.extend(test_df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SAMPLING\n",
    "\n",
    "On supprime cette étape après avoir posé la question du sampling en cours\n",
    "Les textes juridiques contiennent beaucoup de mots rares qui sont précieux pour la compréhension\n",
    "\n",
    "# Garder que 25% pour se focus sur les phrases avec la plus grosse distribution\n",
    "sample_size = int(0.25 * len(train_texts))\n",
    "train_texts_sampled = random.sample(train_texts, sample_size)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Après avoir rassemblé les textes on crée le corpus sur lequel SentencePiece créera la tokenisation\n",
    "# On garde un csv file pour le test\n",
    "\n",
    "with open('train_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in train_texts:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "with open('test_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in test_texts:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de vocabulaire suggérée pour couvrir 97.0% du corpus : 19139\n"
     ]
    }
   ],
   "source": [
    "# Analyser le corpus pour déterminer la taille du vocabulaire\n",
    "all_words = ' '.join(train_texts).split()\n",
    "word_counts = Counter(all_words)\n",
    "total_words = sum(word_counts.values())\n",
    "cumulative_coverage = 0\n",
    "desired_coverage = 0.97  # Par exemple, couvrir 97% du corpus\n",
    "vocab_size = 0\n",
    "\n",
    "for word, count in word_counts.most_common():\n",
    "    cumulative_coverage += count / total_words\n",
    "    vocab_size += 1\n",
    "    if cumulative_coverage >= desired_coverage:\n",
    "        break\n",
    "\n",
    "print(f\"Taille de vocabulaire suggérée pour couvrir {desired_coverage*100}% du corpus : {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrainement \n",
    "spm.SentencePieceTrainer.train(input='train_data.txt', model_prefix='spm_model', vocab_size=vocab_size, model_type='unigram')\n",
    "\n",
    "# Charger le modèle\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data.txt', 'r', encoding='utf-8') as f:\n",
    "    test_texts = f.readlines()\n",
    "\n",
    "tokenized_test_texts = [sp.encode_as_pieces(text.strip()) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Usage: 96.30%\n",
      "Average Token Length: 5.67\n",
      "OOV Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\"\"\"\n",
    "Usage du vocabulaire: vérifier le pourcentage de vocabulaire utilisé dans le test set \n",
    "Taille moyenne d'un token: si la taille est trop petite, cela pourrait indiquer trop de segmentation des mots\n",
    "Out of Vocabulary Rate: on aimerait qu'il soit le plus bas possible\n",
    "\"\"\"\n",
    "\n",
    "all_tokens = [token for text in tokenized_test_texts for token in text]\n",
    "\n",
    "vocab_usage = len(set(all_tokens)) / sp.get_piece_size()\n",
    "print(f\"Vocabulary Usage: {vocab_usage:.2%}\")\n",
    "\n",
    "avg_token_length = sum(len(token) for token in all_tokens) / len(all_tokens)\n",
    "print(f\"Average Token Length: {avg_token_length:.2f}\")\n",
    "\n",
    "oov_tokens = all_tokens.count('<unk>') # est-ce le bon marqueur?\n",
    "oov_rate = oov_tokens / len(all_tokens)\n",
    "print(f\"OOV Rate: {oov_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_statistics(texts):\n",
    "    words = [word for sentence in texts for word in sentence.split()]\n",
    "    word_counts = Counter(words)\n",
    "    n_unique_words = len(word_counts)\n",
    "    n_total_words = sum(word_counts.values())\n",
    "    avg_word_frequency = np.mean(list(word_counts.values()))\n",
    "    return n_unique_words, n_total_words, avg_word_frequency, word_counts\n",
    "\n",
    "def get_sentence_lengths(texts):\n",
    "    sentence_lengths = [len(sentence.split()) for sentence in texts]\n",
    "    avg_sentence_length = np.mean(sentence_lengths)\n",
    "    return sentence_lengths, avg_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Corpus:\n",
      "Unique Words: 304713\n",
      "Total Words: 25093174\n",
      "Average Word Frequency: 82.35\n",
      "Average Sentence Length: 14.02\n"
     ]
    }
   ],
   "source": [
    "train_unique_words, train_total_words, train_avg_word_freq, train_word_counts = get_word_statistics(train_texts)\n",
    "train_sentence_lengths, train_avg_sentence_length = get_sentence_lengths(train_texts)\n",
    "\n",
    "print(\"Training Corpus:\")\n",
    "print(f\"Unique Words: {train_unique_words}\")\n",
    "print(f\"Total Words: {train_total_words}\")\n",
    "print(f\"Average Word Frequency: {train_avg_word_freq:.2f}\")\n",
    "print(f\"Average Sentence Length: {train_avg_sentence_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Corpus:\n",
      "Unique Words: 114961\n",
      "Total Words: 6149586\n",
      "Average Word Frequency: 53.49\n",
      "Average Sentence Length: 13.54\n"
     ]
    }
   ],
   "source": [
    "test_unique_words, test_total_words, test_avg_word_freq, test_word_counts = get_word_statistics(test_texts)\n",
    "test_sentence_lengths, test_avg_sentence_length = get_sentence_lengths(test_texts)\n",
    "\n",
    "print(\"\\nTest Corpus:\")\n",
    "print(f\"Unique Words: {test_unique_words}\")\n",
    "print(f\"Total Words: {test_total_words}\")\n",
    "print(f\"Average Word Frequency: {test_avg_word_freq:.2f}\")\n",
    "print(f\"Average Sentence Length: {test_avg_sentence_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary Overlap between Training and Test: 64.25%\n"
     ]
    }
   ],
   "source": [
    "shared_vocab = set(train_word_counts.keys()).intersection(set(test_word_counts.keys()))\n",
    "overlap_percentage = len(shared_vocab) / min(train_unique_words, test_unique_words)\n",
    "print(f\"\\nVocabulary Overlap between Training and Test: {overlap_percentage:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 1 décret n 76 1073 22 novembr 1976 relat mis protect judiciair travail d intérêt général prononc juridict mineur\n",
      "Tokenized: ['▁1', '▁décret', '▁n', '▁76', '▁10', '73', '▁22', '▁novembr', '▁1976', '▁relat', '▁mis', '▁protect', '▁judiciair', '▁travail', '▁d', '▁intérêt', '▁général', '▁prononc', '▁juridict', '▁mineur']\n",
      "\n",
      "Original: command second d group gendarmer transport aérien\n",
      "\n",
      "Tokenized: ['▁command', '▁second', '▁d', '▁group', '▁gendarmer', '▁transport', '▁aérien']\n",
      "\n",
      "Original: barem national détermin titr indiqu anné conseil supérieur rémuner contrôleur\n",
      "Tokenized: ['▁barem', '▁national', '▁détermin', '▁titr', '▁indiqu', '▁anné', '▁conseil', '▁supérieur', '▁rémuner', '▁contrôleur']\n",
      "\n",
      "Original: lorsqu originair proven pay non membr l union européen envois d animal produit d origin animal produit germinal produit animal produit dériv derni d aliment animal micro organ pathogen animal produit susceptibl véhicul apparten l catégor mention l articl 47 regl ue 2017 625 15 mar 2017 soum frais import moment entré territoir métropolitain guadeloup guyan martin réunion mayott saint martin contrôl officiel post contrôl frontali sen point 38 l articl 3 regl list post contrôl frontali fix arrêt conjoint ministr charg l agricultur ministr charg douan\n",
      "Tokenized: ['▁lorsqu', '▁origin', 'air', '▁proven', '▁pay', '▁non', '▁membr', '▁l', '▁union', '▁européen', '▁envoi', 's', '▁d', '▁animal', '▁produit', '▁d', '▁origin', '▁animal', '▁produit', '▁germinal', '▁produit', '▁animal', '▁produit', '▁dériv', '▁derni', '▁d', '▁aliment', '▁animal', '▁micro', '▁organ', '▁pathogen', '▁animal', '▁produit', '▁susceptibl', '▁véhicul', '▁apparten', '▁l', '▁catégor', '▁mention', '▁l', '▁articl', '▁47', '▁regl', '▁ue', '▁2017', '▁625', '▁15', '▁mar', '▁2017', '▁soum', '▁frais', '▁import', '▁moment', '▁entré', '▁territoir', '▁métropolitain', '▁guadeloup', '▁guyan', '▁martin', '▁réunion', '▁mayott', '▁saint', '▁martin', '▁contrôl', '▁officiel', '▁post', '▁contrôl', '▁frontali', '▁sen', '▁point', '▁38', '▁l', '▁articl', '▁3', '▁regl', '▁list', '▁post', '▁contrôl', '▁frontali', '▁fix', '▁arrêt', '▁conjoint', '▁ministr', '▁charg', '▁l', '▁agricultur', '▁ministr', '▁charg', '▁douan']\n",
      "\n",
      "Original: fr lr arret intf2035151a 2020 12 18\n",
      "Tokenized: ['▁fr', '▁lr', '▁arret', '▁intf', '20351', '51', 'a', '▁2020', '▁12', '▁18']\n",
      "\n",
      "Original: tax acquitt bas titr percept mention 3 l articl 47 derni émis récept l inform mention 2\n",
      "Tokenized: ['▁tax', '▁acquitt', '▁bas', '▁titr', '▁percept', '▁mention', '▁3', '▁l', '▁articl', '▁47', '▁derni', '▁émis', '▁récept', '▁l', '▁inform', '▁mention', '▁2']\n",
      "\n",
      "Original: articl 15 2\n",
      "Tokenized: ['▁articl', '▁15', '▁2']\n",
      "\n",
      "Original: arrêt 22 septembr 2020 port désign préfet coordon sit natur 2000 cavit chauv sour bourgogn gît habitat chauv sour bourgogn zon spécial conserv\n",
      "Tokenized: ['▁arrêt', '▁22', '▁septembr', '▁2020', '▁port', '▁désign', '▁préfet', '▁coordon', '▁sit', '▁natur', '▁2000', '▁cavit', '▁chauv', '▁sour', '▁bourgogn', '▁gît', '▁habitat', '▁chauv', '▁sour', '▁bourgogn', '▁zon', '▁spécial', '▁conserv']\n",
      "\n",
      "Original: résolu 699 17 norm fonction d system d admiss coordin renseign trait sécur maritim utilis l impress band étroit onde décametr\n",
      "Tokenized: ['▁résolu', '▁699', '▁17', '▁norm', '▁fonction', '▁d', '▁system', '▁d', '▁admiss', '▁coordin', '▁renseign', '▁trait', '▁sécur', '▁maritim', '▁utilis', '▁l', '▁impress', '▁band', '▁étroit', '▁onde', '▁déc', 'a', 'metr']\n",
      "\n",
      "Original: tabl\n",
      "Tokenized: ['▁tabl']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_sentences = random.sample(train_texts + test_texts, 10)\n",
    "\n",
    "for sentence in sampled_sentences:\n",
    "    tokenized = sp.encode_as_pieces(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Tokenized: {tokenized}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSE QUALITATIVE\n",
    "\n",
    "- Subword: la stemmisation semble avoir fonctionnée\n",
    "- Nombres: les nombres sont gardés entiers dans les tokens ce qui est important dans ce type de corpus puisque ces nombres ont des significations bien précises (date, numéro d'article etc.)\n",
    "- Les mots comme \"caractéristiques\", \"administration\" ou \"réalité\" sont gardés comme un seul token ce qui préserve leur sémantique. \n",
    "- \"oeuvre\" est correctemment tokenisé donc les caractères spéciales ne représentent pas un problème\n",
    "- Quelques bizzareries: \"îles\" -> \"î\" + \"les\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mot: articl, Tokenisé: ['▁articl']\n",
      "Mot: arrêt, Tokenisé: ['▁arrêt']\n",
      "Mot: présent, Tokenisé: ['▁présent']\n",
      "Mot: servic, Tokenisé: ['▁servic']\n",
      "Mot: ministr, Tokenisé: ['▁ministr']\n",
      "Mot: général, Tokenisé: ['▁général']\n",
      "Mot: charg, Tokenisé: ['▁charg']\n",
      "Mot: disposit, Tokenisé: ['▁disposit']\n",
      "Mot: administr, Tokenisé: ['▁administr']\n",
      "Mot: décret, Tokenisé: ['▁décret']\n",
      "Mot: mention, Tokenisé: ['▁mention']\n",
      "Mot: compt, Tokenisé: ['▁compt']\n",
      "Mot: national, Tokenisé: ['▁national']\n",
      "Mot: aliné, Tokenisé: ['▁aliné']\n",
      "Mot: professionnel, Tokenisé: ['▁professionnel']\n",
      "Mot: remplac, Tokenisé: ['▁remplac']\n",
      "Mot: arret, Tokenisé: ['▁arret']\n",
      "Mot: relat, Tokenisé: ['▁relat']\n",
      "Mot: publiqu, Tokenisé: ['▁publiqu']\n",
      "Mot: établ, Tokenisé: ['▁établ']\n",
      "Mot: appliqu, Tokenisé: ['▁appliqu']\n",
      "Mot: officiel, Tokenisé: ['▁officiel']\n",
      "Mot: prévu, Tokenisé: ['▁prévu']\n",
      "Mot: journal, Tokenisé: ['▁journal']\n",
      "Mot: condit, Tokenisé: ['▁condit']\n",
      "Mot: candidat, Tokenisé: ['▁candidat']\n",
      "Mot: modifi, Tokenisé: ['▁modifi']\n",
      "Mot: français, Tokenisé: ['▁français']\n",
      "Mot: fonction, Tokenisé: ['▁fonction']\n",
      "Mot: public, Tokenisé: ['▁public']\n",
      "Mot: républ, Tokenisé: ['▁républ']\n",
      "Mot: annex, Tokenisé: ['▁annex']\n",
      "Mot: conseil, Tokenisé: ['▁conseil']\n",
      "Mot: social, Tokenisé: ['▁social']\n",
      "Mot: concour, Tokenisé: ['▁concour']\n",
      "Mot: directeur, Tokenisé: ['▁directeur']\n",
      "Mot: décembr, Tokenisé: ['▁décembr']\n",
      "Mot: demand, Tokenisé: ['▁demand']\n",
      "Mot: notair, Tokenisé: ['▁notair']\n",
      "Mot: format, Tokenisé: ['▁format']\n",
      "Mot: societ, Tokenisé: ['▁societ']\n",
      "Mot: susvis, Tokenisé: ['▁susvis']\n",
      "Mot: sécur, Tokenisé: ['▁sécur']\n",
      "Mot: publi, Tokenisé: ['▁publi']\n",
      "Mot: associ, Tokenisé: ['▁associ']\n",
      "Mot: président, Tokenisé: ['▁président']\n",
      "Mot: travail, Tokenisé: ['▁travail']\n",
      "Mot: collect, Tokenisé: ['▁collect']\n",
      "Mot: titulair, Tokenisé: ['▁titulair']\n",
      "Mot: janvi, Tokenisé: ['▁janvi']\n"
     ]
    }
   ],
   "source": [
    "# Analyse des 50 mots les plus fréquents de plus de 4 lettres\n",
    "\n",
    "# Chargement de votre modèle SentencePiece\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')  # Assurez-vous que le chemin est correct\n",
    "\n",
    "# Concaténer tous les textes de votre corpus\n",
    "all_text = ' '.join(train_texts)  # ou test_texts selon le corpus que vous voulez analyser\n",
    "\n",
    "# Compter la fréquence des mots\n",
    "word_freq = Counter(all_text.split())\n",
    "\n",
    "# Filtrer les mots de plus de 4 lettres et obtenir les 50 plus fréquents\n",
    "filtered_words = [word for word in word_freq if len(word) > 4]\n",
    "top_50_words = sorted(filtered_words, key=lambda word: word_freq[word], reverse=True)[:50]\n",
    "\n",
    "# Tokeniser et afficher les mots avec leurs tokenisations\n",
    "for word in top_50_words:\n",
    "    tokenized = sp.encode_as_pieces(word)\n",
    "    print(f\"Mot: {word}, Tokenisé: {tokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mots les plus fréquemment utilisés sont tokenisés en unité distincte ce qui suggère que le modèle a appris efficacement à reconnaitre ces mots communs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
